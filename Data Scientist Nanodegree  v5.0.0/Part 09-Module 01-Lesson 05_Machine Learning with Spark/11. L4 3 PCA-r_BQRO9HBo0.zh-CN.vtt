WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:03.930
当我们想要剔除相关性强的特征来优化特征空间时

00:00:03.930 --> 00:00:07.814
降维技术就十分有用了

00:00:07.815 --> 00:00:13.845
主成分分析或 PCA 是最常用的技术之一

00:00:13.845 --> 00:00:16.394
Spark 也支持 PCA

00:00:16.394 --> 00:00:20.265
它的 feature 库中有对应的内置方法

00:00:20.265 --> 00:00:24.164
我们可以指定我们想要保留的特征数量

00:00:24.164 --> 00:00:27.224
我选择了 100

00:00:27.225 --> 00:00:29.355
我们把它装进 PCA 模式后

00:00:29.355 --> 00:00:31.620
我们就可以对这个 DataFrame 进行转换了

00:00:31.620 --> 00:00:35.340
如你所见 结果是一个 DenseVector

00:00:35.340 --> 00:00:38.250
我们创建了一个叫 pcaTFIDF 的列

00:00:38.250 --> 00:00:40.994
这是它对应的值

00:00:40.994 --> 00:00:45.238
例如，如果您尝试对非常多字段的 DataFrame 使用PCA

00:00:45.238 --> 00:00:49.364
比如在 pcaTFIDF 列中保留10,000个特征

00:00:49.365 --> 00:00:52.265
你可能会遇到内存不足的错误

00:00:52.265 --> 00:00:57.909
只要输入列数不是太高 PCA就可以运行良好

00:00:57.909 --> 00:01:02.599
但是如果有数千个特征 那资源的消耗也将是巨大的

