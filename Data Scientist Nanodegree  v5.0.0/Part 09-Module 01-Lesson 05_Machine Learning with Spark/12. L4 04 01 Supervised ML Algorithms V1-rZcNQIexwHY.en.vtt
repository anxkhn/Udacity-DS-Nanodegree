WEBVTT
Kind: captions
Language: en

00:00:03.980 --> 00:00:08.699
Now that we have created numeric variables of all of our features,

00:00:08.699 --> 00:00:11.439
we can train our first machine learning algorithms.

00:00:11.439 --> 00:00:13.859
First, we will take a look at supervised

00:00:13.859 --> 00:00:17.644
algorithms that we have a labeled training data set.

00:00:17.644 --> 00:00:21.910
Whether we have a classification or a regression task,

00:00:21.910 --> 00:00:23.524
depends on the labels.

00:00:23.524 --> 00:00:26.609
If the labels represent categories, for example,

00:00:26.609 --> 00:00:29.929
users who have upgraded versus users who downgraded,

00:00:29.929 --> 00:00:32.810
we have a classification problem where we need to

00:00:32.810 --> 00:00:36.260
predict the most likely of the predefined set of outcomes.

00:00:36.259 --> 00:00:39.864
If our labels are continuous values, for example,

00:00:39.865 --> 00:00:43.730
how many songs a particular user listens to on a given day,

00:00:43.729 --> 00:00:45.829
we have a regression problem.

00:00:45.829 --> 00:00:48.079
Let's start with classification.

00:00:48.079 --> 00:00:51.019
Spark supports the most common algorithms for

00:00:51.020 --> 00:00:55.925
binary and multi class classification such as logistic regression,

00:00:55.924 --> 00:00:58.820
random forest, gradient boosted trees,

00:00:58.820 --> 00:01:02.215
support vector machines, and Naive Bayes.

00:01:02.215 --> 00:01:05.180
For regression, Spark has built-in methods

00:01:05.180 --> 00:01:08.705
for linear and generalized linear regression,

00:01:08.704 --> 00:01:11.524
and different tree based regressions.

00:01:11.525 --> 00:01:14.450
All these algorithms have detailed documentation

00:01:14.450 --> 00:01:17.719
where you can find out more about their parameters.

00:01:17.719 --> 00:01:21.584
If you would like to transfer some quotes from that say scikit-learn,

00:01:21.584 --> 00:01:26.750
you need to be careful making sure that the parameters have the same interpretation.

00:01:26.750 --> 00:01:31.040
Even then, you might see some performance differences due to

00:01:31.040 --> 00:01:36.510
the differences of loss optimizer implementations across different tools.

