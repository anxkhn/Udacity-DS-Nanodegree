WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.860
在之前的视频中

00:00:01.860 --> 00:00:04.799
我们使用了许多转换器和评估器

00:00:04.799 --> 00:00:08.504
回想一下 转换器使用

00:00:08.505 --> 00:00:12.810
Transfrom 的方法将一个 dataframe 转换为另一个 dataframe

00:00:12.810 --> 00:00:15.945
我们先使用 transform 方法 然后对文本分词

00:00:15.945 --> 00:00:19.410
对一些列的数据进行了缩放 然后把多列数据合并为一个向量

00:00:19.410 --> 00:00:23.295
评估器对数据进行拟合或训练

00:00:23.295 --> 00:00:27.300
它通过 fit方法 输入一个 dataframe

00:00:27.300 --> 00:00:30.780
然后产出一个模型 也就是一个 转换器

00:00:30.780 --> 00:00:35.310
例如 如果我们看一下之前

00:00:35.310 --> 00:00:38.385
视频里训练过的模型的类型

00:00:38.384 --> 00:00:42.210
可以看到它是一个逻辑回归模型

00:00:42.210 --> 00:00:45.439
这是一个转换器

00:00:45.439 --> 00:00:49.309
如果我们检查 lr2 的类型

00:00:49.310 --> 00:00:53.329
你会看到它是逻辑回归

00:00:53.329 --> 00:00:57.155
但它并没有说这是一个评估器

00:00:57.155 --> 00:00:59.990
接下来我们将构建一个管道

00:00:59.990 --> 00:01:03.620
在 Spark 中 与其他框架类似

00:01:03.619 --> 00:01:07.549
例如 scikit-learn ，我们可以把

00:01:07.549 --> 00:01:09.590
转换器和评估器连接在一起

00:01:09.590 --> 00:01:13.130
建立一个完整的管道

00:01:13.129 --> 00:01:17.329
在Spark中 唯一的限制是这些步骤

00:01:17.329 --> 00:01:20.674
需要形成有向无环图

00:01:20.674 --> 00:01:23.989
如果你还记得这个知识点 它指的是

00:01:23.989 --> 00:01:28.114
这个依赖关系中没有任何循环

00:01:28.114 --> 00:01:33.155
所以当我们一步一步搭建管道时

00:01:33.155 --> 00:01:39.394
我们不可以回到我们开始的地方

00:01:39.394 --> 00:01:44.119
那么让我们重新创建逻辑回归模型

00:01:44.120 --> 00:01:46.340
但是这次是建立一条管道

00:01:46.340 --> 00:01:49.265
我将创建一个新的 dataframe

00:01:49.265 --> 00:01:52.790
导入我们一直使用的数据集

00:01:52.790 --> 00:01:56.405
所以我们 dataframe 已经好了

00:01:56.405 --> 00:01:58.280
里面的列我们也很熟悉了

00:01:58.280 --> 00:02:00.890
现在，我们需要定义这个计划的

00:02:00.890 --> 00:02:05.540
所有阶段好了

00:02:05.540 --> 00:02:08.180
我们需要一个 RegexTokenizer

00:02:08.180 --> 00:02:13.105
将问题内容变成一个单词列表

00:02:13.104 --> 00:02:16.500
然后我们创建一个CountVectorizer

00:02:16.500 --> 00:02:22.534
它会将单词转换为单词出现的频率

00:02:22.534 --> 00:02:25.609
在这个例子中 我统计了更多单词

00:02:25.610 --> 00:02:28.730
一共 10,000 字

00:02:28.729 --> 00:02:31.579
然后我们用 IDF 把

00:02:31.580 --> 00:02:34.445
单词频率转换为 TF-IDF

00:02:34.444 --> 00:02:38.074
并将其输出列称为特征列

00:02:38.074 --> 00:02:40.444
这样我们可以节省一步

00:02:40.444 --> 00:02:44.344
把 regexTokenizers CV 和 IDF  连接在一起后

00:02:44.344 --> 00:02:46.055
我们就可以创建特征了

00:02:46.055 --> 00:02:53.510
我们还需要将字符串标签转换为数值

00:02:53.509 --> 00:02:55.594
我们将这列叫做 label

00:02:55.594 --> 00:03:00.004
然后 我们可以定义逻辑回归模型了

00:03:00.004 --> 00:03:03.444
我们将使用和之前相同的参数

00:03:03.444 --> 00:03:07.655
然后最后一步是定义一个管道对象

00:03:07.655 --> 00:03:10.610
为防止步骤相互依赖

00:03:10.610 --> 00:03:12.800
我们需要维持他们的顺序

00:03:12.800 --> 00:03:16.630
例如 这里我们需要从 regexTokenizer 开始

00:03:16.629 --> 00:03:18.734
然后是 countVectorizer

00:03:18.735 --> 00:03:22.305
最后我们再加上 idf

00:03:22.305 --> 00:03:27.170
由于 stringIndexer 是把文本转换为数值

00:03:27.169 --> 00:03:30.919
它与之前的这些步骤无关

00:03:30.919 --> 00:03:34.280
我们也可以把 indexer 放在最开头

00:03:34.280 --> 00:03:39.094
但是我选择把它放在这三个步骤之后

00:03:39.094 --> 00:03:44.750
最后我们把模型本身放进去

00:03:44.750 --> 00:03:46.789
既然我们的管道创建好了

00:03:46.789 --> 00:03:51.289
我们只需要拟合它了我们的模型已经好了

00:03:51.289 --> 00:03:54.620
结果会返回一个转换器

00:03:54.620 --> 00:03:57.319
用来对数据做预测

00:03:57.319 --> 00:04:02.150
我先用训练数据来看看

00:04:02.150 --> 00:04:03.955
预测的标签是什么

00:04:03.955 --> 00:04:06.555
看一下第一个记录

00:04:06.555 --> 00:04:10.069
我们可以看到有两个新列

00:04:10.069 --> 00:04:13.549
这个是概率列

00:04:13.550 --> 00:04:18.395
以及预测本身

00:04:18.394 --> 00:04:21.019
我们看一下上面

00:04:21.019 --> 00:04:26.384
你可以看到第三个标签的概率

00:04:26.384 --> 00:04:30.875
计数从零开始 这个概率非常高了

00:04:30.875 --> 00:04:32.720
大概是85％

00:04:32.720 --> 00:04:36.725
所以这就是我们预测的标签

00:04:36.725 --> 00:04:44.395
那么我们来看看这个模型预测效果如何

00:04:44.394 --> 00:04:48.539
在这里 我用 filter  把

00:04:48.540 --> 00:04:51.660
标签和预测标签相同的行给过滤出来

00:04:51.660 --> 00:04:54.945
然后计算这些行的数量

00:04:54.944 --> 00:05:01.134
在我们的 100,000 行中

00:05:01.134 --> 00:05:04.969
超过一半的预测是对的

00:05:04.970 --> 00:05:07.370
但当然 这个准确率

00:05:07.370 --> 00:05:10.129
是在训练集上测量的

00:05:10.129 --> 00:05:13.519
所以效果会偏好

00:05:13.519 --> 00:05:16.279
在下一节中 我们会看看

00:05:16.279 --> 00:05:21.349
在如何正确衡量模型效果

