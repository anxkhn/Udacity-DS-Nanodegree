WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:01.754
在下面的几个视频中

00:00:01.754 --> 00:00:06.764
我们要使用技术交流论坛的数据集 这个数据集包括了很多在这个论坛上被大家讨论的问题

00:00:06.764 --> 00:00:09.779
每个问题都包含 ID 、 标题字段

00:00:09.779 --> 00:00:14.519
和问题的内容字段以及问题的标签字段

00:00:14.519 --> 00:00:17.759
我们会使用这个数据集创建一些特征

00:00:17.760 --> 00:00:20.220
并训练各种机器学习模型

00:00:20.219 --> 00:00:21.524
注意

00:00:21.524 --> 00:00:26.174
这些例子的目的不是教你找最好的特征

00:00:26.175 --> 00:00:28.574
或是训练最准确的模型

00:00:28.574 --> 00:00:32.189
我会向你展示一些通用技术

00:00:32.189 --> 00:00:35.669
以便你在大型数据集上计算特征和模型

00:00:35.670 --> 00:00:41.655
课程中的许多例子都深受 Spark 文档的启发

00:00:41.655 --> 00:00:43.300
我希望

00:00:43.299 --> 00:00:45.844
你正在观看这些视频时

00:00:45.844 --> 00:00:48.679
能打开官方文档去看看这些部分的内容

00:00:48.679 --> 00:00:54.774
你在 Spark ML 库里将找到更多相关信息

00:00:54.774 --> 00:00:58.164
首先 我先导入一些待会儿要用的函数

00:00:58.164 --> 00:01:01.129
然后把数据在

00:01:01.130 --> 00:01:03.440
S3 bucket 中存储的路径写好

00:01:03.439 --> 00:01:07.384
该文件名为 “ Train_onetag_small.jason ”

00:01:07.385 --> 00:01:13.425
这是文件中的一个小样本 大约有 100,000 行

00:01:13.424 --> 00:01:17.299
我用小数据集的原因是

00:01:17.299 --> 00:01:21.259
减少在视频里的计算时间

00:01:21.260 --> 00:01:24.650
不过我在这里展示的东西也适用于更大的数据集

00:01:24.650 --> 00:01:28.475
这个数据集以 json 格式存储的

00:01:28.474 --> 00:01:35.944
所以 我们可以使用 read.json 方法将数据集加载到名为 df 的 Spark DataFrame 中

00:01:35.944 --> 00:01:38.134
DataFrame 已经创建好了

00:01:38.135 --> 00:01:40.730
你可以看到不同的列

00:01:40.730 --> 00:01:46.585
问题内容、 ID 、 标签 、 标题 和oneTag

00:01:46.584 --> 00:01:49.259
如果我们看一下 DataFrame 的头行

00:01:49.260 --> 00:01:52.425
头行的问题是

00:01:52.424 --> 00:01:56.674
“ 如何检查上传的文件是否是没有 mime 类型的图像”

00:01:56.674 --> 00:01:58.804
它有几个标签

00:01:58.805 --> 00:02:04.950
php图像处理，文件上传，上传和 mime 类型

00:02:04.950 --> 00:02:07.980
第一步 我们要创建一些特征

00:02:07.980 --> 00:02:12.030
我们先从问题描述的长度开始

00:02:12.030 --> 00:02:17.444
要计算这个 首先你要把文本分解为单词

00:02:17.444 --> 00:02:21.120
我们可以使用 ML 库 里的 regexTokenizer

00:02:21.120 --> 00:02:26.705
它只保留文本里的单词并把所有字母改成小写

00:02:26.705 --> 00:02:30.005
其他标点符号或特殊字符都会被去掉

00:02:30.004 --> 00:02:33.588
这个模式里的参数就是这个意思

00:02:33.588 --> 00:02:40.504
我们通过 body 列创建了一个新列 叫做 words

00:02:40.504 --> 00:02:42.544
你看第一行

00:02:42.544 --> 00:02:47.159
确实有一个新的字段 words 被创建了

00:02:47.159 --> 00:02:52.189
它包含了问题主体中的所有单词

00:02:52.189 --> 00:02:55.419
某些情况下 例如 I’d

00:02:55.419 --> 00:03:03.659
因为我们把所有的标点符号去掉了 所以 i 和 d 就分开了

00:03:03.659 --> 00:03:08.829
现在 我们可以写个自定义函数来计算这个列表的长度

00:03:08.830 --> 00:03:12.105
我们把这列叫做为 body_length

00:03:12.104 --> 00:03:15.590
我们要用下划线而不是破折号

00:03:15.590 --> 00:03:19.694
我们创建了这个自定义函数后 接下来我们会

00:03:19.694 --> 00:03:20.989
把这个函数用在 dataframe 中的 words 列上

00:03:20.990 --> 00:03:26.615
从而创造出一个名为 “BodyLength" 的列

00:03:26.615 --> 00:03:28.189
我们来看一下

00:03:28.189 --> 00:03:29.689
正如你所看到的

00:03:29.689 --> 00:03:33.099
第一行的问题内容的长度是83

00:03:33.099 --> 00:03:38.409
我们也可以使用 Python regex 库写一些正则表达式

00:03:38.409 --> 00:03:44.020
来看看内容中有多少段落或链接

00:03:44.020 --> 00:03:49.875
我们可以使用段落和链接结束标记作为参考

00:03:49.875 --> 00:03:51.639
比如这个

00:03:51.639 --> 00:03:55.309
我们写了一个 这样的正则表达

00:03:55.310 --> 00:03:58.789
寻找这个结束标签的出现次数

00:03:58.789 --> 00:04:04.009
我们把 NumParagraphs 和 NumLinks 列添加到 DataFrame 中

00:04:04.009 --> 00:04:05.870
如果再看一下头行

00:04:05.870 --> 00:04:10.879
这一行的 NumParagraphs 的值变成了 2 

00:04:10.879 --> 00:04:16.894
NumLinks 的值为 0

00:04:16.894 --> 00:04:19.189
现在我们有数值型的特征

00:04:19.189 --> 00:04:22.069
对于许多模型 我们需要标准化特征的值

00:04:22.069 --> 00:04:24.925
确保它们有相同的范围

00:04:24.925 --> 00:04:26.889
不然的话 距离函数会把

00:04:26.889 --> 00:04:30.649
具有最高值的特征对结果的影响放大

00:04:30.649 --> 00:04:36.924
Sparks ML 库有几个选项 例如 normalizers standard scaler 和 MSQL

00:04:36.925 --> 00:04:40.384
所有这些函数都需要向量行作为输入

00:04:40.384 --> 00:04:42.004
所以 在我们使用这些函数之前

00:04:42.004 --> 00:04:46.399
需要将数值型数据转换为 Sparks 向量类型

00:04:46.399 --> 00:04:51.185
我们可以使用 Spark ML feature 库中的 VectorAssembler 

00:04:51.185 --> 00:04:54.530
我会给你看看 StandardScaler 和 Normalizer

00:04:54.529 --> 00:04:58.250
用在这三个特征上的效果

00:04:58.250 --> 00:05:01.670
BodyLength NumParagraphs 和 NumLinks

00:05:01.670 --> 00:05:04.525
正如我们在第一条记录中看到的

00:05:04.524 --> 00:05:09.724
NumFeatures 现在是一个包含这三个值的 DenseVector

00:05:09.725 --> 00:05:13.250
问题中的单词数

00:05:13.250 --> 00:05:16.185
段落数以及链接数

00:05:16.185 --> 00:05:21.314
接下来我们看看使用 normalizer 会发生什么

00:05:21.314 --> 00:05:27.350
Normalizer 可以对数据集或向量行进行转化

00:05:27.350 --> 00:05:30.785
把每个向量归一化

00:05:30.785 --> 00:05:33.830
正如你在这里看到的

00:05:33.829 --> 00:05:40.479
normalizer 会保持这些值的比率 现在它们总和为1

00:05:40.480 --> 00:05:46.925
StandardScaler 将特征标准化为单位标准差是0均值 也可以是0均值单位标准差

00:05:46.925 --> 00:05:51.715
取决于参数 withStd 和 withMean 的值

00:05:51.714 --> 00:05:54.334
我们看了前几行之后

00:05:54.334 --> 00:05:56.659
你可以清楚地看到差异

00:05:56.660 --> 00:05:59.150
例如 在第一个纪录里

00:05:59.149 --> 00:06:04.584
scaledNumFeatures 长这样 我们之前已经看到过了

00:06:04.584 --> 00:06:07.319
使用 StandardScaler 后

00:06:07.319 --> 00:06:09.959
我们得到了这个结果

00:06:09.959 --> 00:06:13.569
这是第二行的例子

