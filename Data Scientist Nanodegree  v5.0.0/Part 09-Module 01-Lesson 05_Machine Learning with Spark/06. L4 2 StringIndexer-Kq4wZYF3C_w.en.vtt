WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.669
Then when we tokenize text,

00:00:02.669 --> 00:00:10.109
we often use either the frequency of the words as a feature or even better to TFIDF,

00:00:10.109 --> 00:00:12.209
Term Frequency Inverse Document

00:00:12.210 --> 00:00:16.635
Frequency that represents the relative specificity over word.

00:00:16.635 --> 00:00:20.250
Spark has built the methods to compute both of these.

00:00:20.250 --> 00:00:24.225
We can use the CountVectorizer to get the word counts.

00:00:24.225 --> 00:00:27.510
We can set how many words we would like to keep.

00:00:27.510 --> 00:00:31.874
Here I chose a vocabSize of 1,000,

00:00:31.873 --> 00:00:35.714
meaning that we'll keep the top 1,000 most common words.

00:00:35.715 --> 00:00:39.660
Spark stores the word counts as a sparse vector.

00:00:39.659 --> 00:00:45.044
So for example, the first record has the 0th word,

00:00:45.045 --> 00:00:47.295
four times the first one,

00:00:47.295 --> 00:00:49.740
six times the second one,

00:00:49.740 --> 00:00:52.289
two times and so far so on.

00:00:52.289 --> 00:00:58.039
The calling vocabulary, we can see the list of words we use in the CV model.

00:00:58.039 --> 00:01:01.729
Here you can see the first few in

00:01:01.729 --> 00:01:05.780
contractariser the zeros in next goes to the most common word.

00:01:05.780 --> 00:01:07.019
So in this case,

00:01:07.019 --> 00:01:12.819
this is p. If you remember the HTML tag for a paragraph of a space,

00:01:12.819 --> 00:01:16.674
so probably that's why the CP is open.

00:01:16.674 --> 00:01:19.549
There are some other very common words,

00:01:19.549 --> 00:01:22.679
for example, 'the', 'a', 'code.'

00:01:22.680 --> 00:01:25.394
Lots of calling questions it seems,

00:01:25.394 --> 00:01:29.234
'is' 'and' and similar words like that.

00:01:29.234 --> 00:01:32.560
We can also take a look at the least common ones.

00:01:32.560 --> 00:01:35.700
Here we see less common words such as customer,

00:01:35.700 --> 00:01:39.049
desktop, buttons and a few others.

00:01:39.049 --> 00:01:41.465
Then we compute the TFIDF,

00:01:41.465 --> 00:01:46.329
we can see that the relative frequency of the words at the beginning of the vector,

00:01:46.329 --> 00:01:49.319
so the 0th first,

00:01:49.319 --> 00:01:53.179
these are the words that are the most common in our data set has dropped.

00:01:53.180 --> 00:01:59.440
So, TFIDF effectively surpasses the stop words as we expected.

00:01:59.439 --> 00:02:05.629
As the last step, let's turn the one tag field that contains strings into numeric values.

00:02:05.629 --> 00:02:09.289
The one tag field contains the most common tag,

00:02:09.289 --> 00:02:14.275
calculated over the whole data set of the tags of a particular question.

00:02:14.275 --> 00:02:18.164
So for example, if a question was tagged with Java and load,

00:02:18.164 --> 00:02:19.564
I can tag Java,

00:02:19.564 --> 00:02:22.025
since it's more common than the tag load.

00:02:22.025 --> 00:02:24.715
In this previous example,

00:02:24.715 --> 00:02:28.694
among the tags PHP image processing file upload,

00:02:28.694 --> 00:02:33.120
upload mime-types, the one tag column

00:02:33.120 --> 00:02:39.115
became PHP since this was the most common among these five tags.

00:02:39.115 --> 00:02:41.250
We can use the string indexing method,

00:02:41.250 --> 00:02:43.530
to do this transformation.

00:02:43.530 --> 00:02:45.780
Looking at this example,

00:02:45.780 --> 00:02:50.534
the tag PHP became label three.

00:02:50.534 --> 00:02:53.389
String indexer gives the 0th index and

00:02:53.389 --> 00:02:57.049
the most common string and then goes in ascending order.

00:02:57.050 --> 00:03:00.950
So, PHP is one of the most common tags in our data set.

00:03:00.949 --> 00:03:09.614
The reason why we might want to transform a string value to a numeric one is because

00:03:09.615 --> 00:03:13.860
Spark similarly to other frameworks need numeric values both

00:03:13.860 --> 00:03:18.530
as features and targets to compute machine-learning models.

