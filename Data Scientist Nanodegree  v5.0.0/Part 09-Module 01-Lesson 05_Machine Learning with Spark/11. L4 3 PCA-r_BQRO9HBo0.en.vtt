WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.930
Dimensionality reduction techniques can be really useful when we

00:00:03.930 --> 00:00:07.814
would like to remove correlated features and shrink the feature space.

00:00:07.815 --> 00:00:13.845
Principal Component Analysis or PCA is one of the most commonly used techniques,

00:00:13.845 --> 00:00:16.394
Spark supports PC as well,

00:00:16.394 --> 00:00:20.265
it has a built-in method in the featured library.

00:00:20.265 --> 00:00:24.164
We can specify how many components we would like to keep,

00:00:24.164 --> 00:00:27.224
here I chose 100.

00:00:27.225 --> 00:00:29.355
Once it fit the PCA mode,

00:00:29.355 --> 00:00:31.620
we can transform our DataFrame.

00:00:31.620 --> 00:00:35.340
As you can see, the result is a DenseVector.

00:00:35.340 --> 00:00:38.250
Re-created the column PCA TFIDF,

00:00:38.250 --> 00:00:40.994
and here are the values.

00:00:40.994 --> 00:00:45.238
If you try to use PCA on a DataFrame, for example,

00:00:45.238 --> 00:00:49.364
if he kept 10,000 features in TFIDF,

00:00:49.365 --> 00:00:52.265
you might run into out-of-memory errors.

00:00:52.265 --> 00:00:57.909
BCA works well, as long as the number of input columns is not too high,

00:00:57.909 --> 00:01:02.599
but can get quite resource-intensive with thousands of features.

