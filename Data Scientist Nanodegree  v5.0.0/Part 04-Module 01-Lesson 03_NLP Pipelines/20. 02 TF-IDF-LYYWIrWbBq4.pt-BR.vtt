WEBVTT
Kind: captions
Language: pt-BR

00:00:00.132 --> 00:00:03.336
Uma limitação da abordagem
do saco de palavras

00:00:03.370 --> 00:00:06.539
é que ele trata cada palavra
como sendo igualmente importante.

00:00:06.573 --> 00:00:11.678
Sabemos que algumas palavras ocorrem
frequentemente dentro de um corpus.

00:00:11.711 --> 00:00:14.681
Por exemplo, ao olhar
documentos financeiros,

00:00:14.714 --> 00:00:17.684
vemos que "custo" ou "preço"
podem ser termos bem comuns.

00:00:17.717 --> 00:00:21.354
Podemos compensar isto contando
o número de documentos

00:00:21.388 --> 00:00:23.089
em que cada palavra ocorre.

00:00:23.123 --> 00:00:25.291
Isto se chama
"frequência de documento".

00:00:25.325 --> 00:00:27.660
E dividindo
as frequências dos termos

00:00:27.694 --> 00:00:30.363
pela frequência do documento
desse termo,

00:00:30.397 --> 00:00:34.467
temos uma métrica proporcional
à frequência de ocorrência

00:00:34.501 --> 00:00:36.002
de um termo num documento,

00:00:36.036 --> 00:00:37.771
mas inversamente proporcional

00:00:37.804 --> 00:00:40.106
ao número de documentos
em que ele aparece.

00:00:40.140 --> 00:00:43.943
Isso destaca as palavras que são
mais distintas a um documento,

00:00:43.977 --> 00:00:46.413
o que é melhor
para a caracterização.

00:00:46.446 --> 00:00:51.017
Você já deve ter ouvido falar
ou usado a transformação TF-IDF.

00:00:51.051 --> 00:00:53.553
É simplesmente o produto
de dois pesos,

00:00:53.586 --> 00:00:55.889
bem similar
ao que vimos até agora,

00:00:55.922 --> 00:00:59.893
a frequência do termo
e a frequência de documento inversa.

00:00:59.926 --> 00:01:04.698
A forma mais usada da TF-IDF
define frequência de termo

00:01:04.731 --> 00:01:08.568
como a contagem bruta
de um termo "t" num documento "d"

00:01:08.601 --> 00:01:11.371
dividida pelo número total
de termos em d,

00:01:11.404 --> 00:01:15.175
e frequência de documento inversa
como o logaritmo

00:01:15.208 --> 00:01:18.812
do número total de documentos
na coleção D

00:01:18.845 --> 00:01:22.215
dividido pelo número de documentos
onde t está presente.

00:01:22.248 --> 00:01:24.117
Existem muitas variações

00:01:24.150 --> 00:01:27.387
que tentam normalizar,
suavizar os valores resultantes

00:01:27.420 --> 00:01:30.790
ou prevenir casos extremos
como dividir por zero erros.

00:01:30.824 --> 00:01:33.727
No geral, TF-IDF é
uma abordagem inovadora

00:01:33.760 --> 00:01:35.528
para atribuir pesos
a palavras,

00:01:35.562 --> 00:01:38.098
que significam
sua relevância em documentos.

00:01:38.131 --> 00:01:43.570
Mas o que fazer com saco de palavras
ou representação TF-IDF?

00:01:43.603 --> 00:01:45.672
Vamos tentar aplicar
o que aprendemos

00:01:45.705 --> 00:01:47.607
em uma tarefa
de classificação,

00:01:47.640 --> 00:01:51.478
distinguindo mensagens de spam,
de mensagens normais.

00:01:51.511 --> 00:01:54.848
Aqui vamos usar os vetores
TF-IDF como recursos,

00:01:54.881 --> 00:01:57.050
junto com os rótulos "spam"
e "não spam",

00:01:57.083 --> 00:01:59.986
para configurar um problema
de aprendizado supervisionado.

