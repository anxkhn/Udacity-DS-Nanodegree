WEBVTT
Kind: captions
Language: zh-CN

00:00:05.099 --> 00:00:09.794
另一个你会面临的问题是重复数据

00:00:09.794 --> 00:00:12.820
你可能认为重复数据就是

00:00:12.820 --> 00:00:16.245
数据集中出现了相同的行

00:00:16.245 --> 00:00:18.670
这很常见

00:00:18.670 --> 00:00:22.880
这种情况下 你可以找到重复的行然后删除它们

00:00:22.879 --> 00:00:27.644
我们可以用 pandas 的 drop_duplicates 方法 方便地实现这一目的

00:00:27.644 --> 00:00:33.159
然而 重复数据有时并不那么显而易见

00:00:33.159 --> 00:00:36.989
比如说 你的公司有几份不同的联系方式列表

00:00:36.990 --> 00:00:39.329
并且想把它合并起来

00:00:39.329 --> 00:00:43.085
一份联系列表包括联系人的名（first name）

00:00:43.085 --> 00:00:47.765
姓（last name） 电话和邮箱地址

00:00:47.765 --> 00:00:51.655
表格看起来是这样

00:00:51.655 --> 00:00:55.204
第二份列表包括联系人的名

00:00:55.204 --> 00:00:58.530
姓 和邮箱地址

00:00:58.530 --> 00:01:01.740
这是第二份联系方式列表

00:01:01.740 --> 00:01:05.765
一个用户的名字是 Cristina Real

00:01:05.765 --> 00:01:08.075
在第一份联系方式列表中

00:01:08.075 --> 00:01:12.365
她的名字显示为 Cristina Real

00:01:12.364 --> 00:01:14.304
在第二份列表中

00:01:14.305 --> 00:01:18.065
她的名字显示为 Christina Real

00:01:18.064 --> 00:01:23.539
多了一个 h 如果你直接基于姓名

00:01:23.540 --> 00:01:26.665
合并两张表

00:01:26.665 --> 00:01:31.425
会得到两行 Cristina 的信息 而不是一行

00:01:31.424 --> 00:01:34.864
这是一个重复数据的例子

00:01:34.864 --> 00:01:41.914
认识到 Cristina 和多了一个 h 的 Christina 其实是同一个人 并不是那么容易做到的

00:01:41.915 --> 00:01:46.400
目前有软件项目和公司专门致力于

00:01:46.400 --> 00:01:51.925
找到和减少这些情况下出现的重复行

00:01:51.924 --> 00:01:54.829
缺失数据只是一个例子

00:01:54.829 --> 00:01:57.829
每个数据集都有自己的问题

00:01:57.829 --> 00:02:02.859
作为数据科学家或者工程师 你的一部分职责就是梳理数据

00:02:02.859 --> 00:02:04.515
定位问题所在

00:02:04.515 --> 00:02:07.519
编写代码修复这些问题

