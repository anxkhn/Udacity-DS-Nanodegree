WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.580
Here is a simple example that uses

00:00:02.580 --> 00:00:06.135
grid search to find parameters for support vector classifier.

00:00:06.135 --> 00:00:10.320
All you need to do, is create a dictionary to define your parameter grid.

00:00:10.320 --> 00:00:13.775
Using the keys for the names of the parameters,

00:00:13.775 --> 00:00:17.925
and the values for the list of values to check during grid search.

00:00:17.925 --> 00:00:22.505
Then, pass your model and parameter grid to the grid search object.

00:00:22.505 --> 00:00:24.810
Calling fit on the grid search object,

00:00:24.810 --> 00:00:28.340
we'll run cross-validation on all different combinations of

00:00:28.340 --> 00:00:32.905
these parameters to find the best combination of parameters for this model.

00:00:32.905 --> 00:00:36.410
Now consider if we had a data preprocessing step,

00:00:36.410 --> 00:00:39.305
where we standardize the data using standard scaler.

00:00:39.305 --> 00:00:41.490
This may seem okay at first.

00:00:41.490 --> 00:00:44.660
But if you standardize your whole training set,

00:00:44.660 --> 00:00:47.745
and then use cross validation to evaluate your model,

00:00:47.745 --> 00:00:49.640
you've got data leakage.

00:00:49.640 --> 00:00:54.945
Let me explain, grid search uses cross-validation to score your model.

00:00:54.945 --> 00:00:58.910
Splitting your data into training and validation sets,

00:00:58.910 --> 00:01:03.025
training your model on the training set and then scoring it on the validation set,

00:01:03.025 --> 00:01:04.910
and it does this multiple times.

00:01:04.910 --> 00:01:07.285
However, each time this happens,

00:01:07.285 --> 00:01:11.135
the model already has knowledge of the validation set because

00:01:11.135 --> 00:01:16.280
all the data was re-scaled based on the distribution of the whole training dataset.

00:01:16.280 --> 00:01:18.530
Important factors like the mean and

00:01:18.530 --> 00:01:21.980
standard deviation are influenced by all the training data,

00:01:21.980 --> 00:01:26.155
meaning the model may perform better than it really should on unseen data.

00:01:26.155 --> 00:01:28.610
Since the information from the validation set

00:01:28.610 --> 00:01:31.525
is already baked in into the re-scaled values.

00:01:31.525 --> 00:01:36.410
The way to fix this would be to make sure that you run standard scalar only on

00:01:36.410 --> 00:01:41.375
the training set and not the validation set within each fold of cross-validation.

00:01:41.375 --> 00:01:43.845
Pipelines allow you to do just this.

00:01:43.845 --> 00:01:48.275
Now, since the re-scaling is included as part of the pipeline.

00:01:48.275 --> 00:01:52.280
The standardization doesn't happen until we run grid search.

00:01:52.280 --> 00:01:55.060
Meaning in each fold of cross-validation,

00:01:55.060 --> 00:01:58.640
re-scaling is only done on the data that the model

00:01:58.640 --> 00:02:02.255
is being trained on preventing leakage from the validation set.

00:02:02.255 --> 00:02:06.900
In addition, you can also grid search values within these data transformation steps.

00:02:06.900 --> 00:02:10.220
As you can see, pipelines are very valuable to removing

00:02:10.220 --> 00:02:14.000
the risk of data leakage during the data preparation process.

