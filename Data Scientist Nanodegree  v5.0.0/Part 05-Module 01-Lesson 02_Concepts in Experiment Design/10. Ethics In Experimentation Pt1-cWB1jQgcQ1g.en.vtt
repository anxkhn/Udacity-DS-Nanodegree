WEBVTT
Kind: captions
Language: en

00:00:04.040 --> 00:00:08.190
There's one more major topic to discuss before closing out

00:00:08.189 --> 00:00:12.300
this lesson and that's Ethics and Experimentation.

00:00:12.300 --> 00:00:15.900
Even if you have the rest of your studies designed figured out,

00:00:15.900 --> 00:00:18.359
you should not run it until you've gone through it and

00:00:18.359 --> 00:00:21.669
checked out any ethical issues in the design.

00:00:21.670 --> 00:00:25.554
Briefly there are four main principles to consider.

00:00:25.554 --> 00:00:28.350
First and foremost, consider the risks

00:00:28.350 --> 00:00:31.515
to users for their participation in the experiment.

00:00:31.515 --> 00:00:34.789
As a researcher, you have an obligation to

00:00:34.789 --> 00:00:38.314
minimize the potential harm exposed to a participant.

00:00:38.314 --> 00:00:43.974
It's important to remember that emotional distress is also a form of harm.

00:00:43.975 --> 00:00:48.079
In 2014, a Facebook's study manipulated the content of

00:00:48.079 --> 00:00:52.114
users' News Feeds to see how it affected their posting patterns.

00:00:52.115 --> 00:00:55.175
Some users saw slightly more positive posts

00:00:55.174 --> 00:00:58.570
and some users saw slightly more negative posts.

00:00:58.570 --> 00:01:01.850
The study violated the principle of minimizing harm,

00:01:01.850 --> 00:01:05.210
since it was not clear how severe the effects the manipulation

00:01:05.209 --> 00:01:09.629
would have on users mental or emotional states.

00:01:09.629 --> 00:01:13.734
Secondly, you should consider the benefits of the experiment,

00:01:13.734 --> 00:01:18.469
especially relative to the possible harms to the study participants.

00:01:18.469 --> 00:01:22.504
The Facebook's study was also in violation of this principle,

00:01:22.504 --> 00:01:24.530
since there wasn't a clear benefit for

00:01:24.530 --> 00:01:27.950
the research that the experiment was driving towards.

00:01:27.950 --> 00:01:30.620
If there is possibility for harm,

00:01:30.620 --> 00:01:35.040
it needs to be clear that the benefits are worth the risks taken.

